# 使用 Kubectl 部署 NebulaGraph 集群

!!! Compatibility "历史版本兼容性"

    1.x 版本的 NebulaGraph Operator 不兼容 3.x 以下版本的 NebulaGraph。

## 前提条件

- [安装 NebulaGraph Operator](../2.deploy-nebula-operator.md)

{{ ent.ent_begin }}
- 准备相应的 License 文件

  !!! enterpriseonly

        只有当创建的 NebulaGraph 集群是企业版本时，才需要提供 License 文件。
{{ ent.ent_end }}

## 创建集群

本文以创建名为`nebula`的集群为例，说明如何部署 NebulaGraph 集群。

1. 社区版集群示例，创建名为`apps_v1alpha1_nebulacluster.yaml`的文件。
  
  ```yaml
  apiVersion: apps.nebula-graph.io/v1alpha1
  kind: NebulaCluster
  metadata:
    name: nebula
  spec:
    graphd:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 1
      image: vesoft/nebula-graphd
      version: {{nebula.tag}}
      service:
        type: NodePort
        externalTrafficPolicy: Local
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
    metad:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 1
      image: vesoft/nebula-metad
      version: {{nebula.tag}}
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
    storaged:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 3
      image: vesoft/nebula-storaged
      version: {{nebula.tag}}
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
  reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
    imagePullPolicy: Always
  ```

  参数描述如下：

  | 参数    | 默认值  | 描述    |
  | :---- | :--- | :--- |
  | `metadata.name`              | -                                                            | 创建的 NebulaGraph 集群名称。 |
  | `spec.graphd.replicas`       | `1`                                                          | Graphd 服务的副本数。         |
  | `spec.graphd.images`         | `vesoft/nebula-graphd`                                       | Graphd 服务的容器镜像。       |
  | `spec.graphd.version`        | `{{nebula.tag}}`                                                     | Graphd 服务的版本号。         |
  | `spec.graphd.service`        | -                                                            | Graphd 服务 Service 配置。      |
  | `spec.graphd.logVolumeClaim.storageClassName`   | -                                                            | Graphd 服务的日志盘存储配置。         |
  | `spec.metad.replicas`        | `1`                                                          | Metad 服务的副本数。          |
  | `spec.metad.images`          | `vesoft/nebula-metad`                                        | Metad 服务的容器镜像。        |
  | `spec.metad.version`         | `{{nebula.tag}}`                                                     | Metad 服务的版本号。          |
  | `spec.metad.dataVolumeClaim.storageClassName`    | -                                                            | Metad 服务的数据盘存储配置。          |
  | `spec.metad.logVolumeClaim.storageClassName`|-|Metad 服务的日志盘存储配置。|
  | `spec.storaged.replicas`     | `3`                                                          | Storaged 服务的副本数。       |
  | `spec.storaged.images`       | `vesoft/nebula-storaged`                                     | Storaged 服务的容器镜像。     |
  | `spec.storaged.version`      | `{{nebula.tag}}`                                                     | Storaged 服务的版本号。       |
  | `spec.storaged.dataVolumeClaim.storageClassName` | -                                                            | Storaged 服务的数据盘存储配置。       |
  | `spec.storaged.logVolumeClaim.storageClassName`|-|Storaged 服务的日志盘存储配置。|
  | `spec.reference.name`        | -                                                            | 依赖的控制器名称。           |
  | `spec.schedulerName`         | -                                                            | 调度器名称。                 |
  | `spec.imagePullPolicy`       | NebulaGraph 镜像的拉取策略。关于拉取策略详情，请参考 [Image pull policy](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy)。 | 镜像拉取策略。               |


  {{ ent.ent_begin }}
1. 企业版集群示例，创建名为`apps_v1alpha1_nebulacluster.yaml`的文件。

  ```yaml
  # 联系销售人员获取完整配置。
  apiVersion: apps.nebula-graph.io/v1alpha1
  kind: NebulaCluster
  metadata:
    annotations:
      nebula-graph.io/owner: test
    name: nebula
  spec:
    enablePVReclaim: true
    graphd:
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /status
          port: 19669
          scheme: HTTP
        initialDelaySeconds: 40
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      image: reg.vesoft-inc.com/vesoft-ent/nebula-graphd
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      replicas: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      version: {{nebula.tag}}
    imagePullPolicy: Always
    imagePullSecrets:
    - name: vesoft
    metad:
      license:
        secretName: nebula-license
        licenseKey: nebula.license
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /status
          port: 19559
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      image: reg.vesoft-inc.com/vesoft-ent/nebula-metad
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      replicas: 1
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      version: {{nebula.tag}}
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
    storaged:
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /status
          port: 19779
          scheme: HTTP
        initialDelaySeconds: 40
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      image: reg.vesoft-inc.com/vesoft-ent/nebula-storaged
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: fast-disks
      replicas: 3
      resources:
        limits:
          cpu: "1"
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 500Mi
      version: {{nebula.tag}}
      enableAutoBalance: true      
  ```

  企业版特有参数描述如下：

  | 参数    | 默认值  | 描述    |
  | :---- | :--- | :--- |
  | `spec.metad.license`       | - | 创建企业版 NebulaGraph 集群所需的 License 配置。               | 

  !!! enterpriseonly

        拉取企业版 NebulaGraph 集群镜像前，请确保已联系销售人员（[inqury@vesoft.com](mailto:inqury@vesoft.com)）获取企业版 NebulaGraph 集群的镜像信息。
  {{ ent.ent_end }}

2. 创建 NebulaGraph 集群。

  ```bash
  kubectl create -f apps_v1alpha1_nebulacluster.yaml
  ```

  返回：

  ```bash
  nebulacluster.apps.nebula-graph.io/nebula created
  ```

  {{ ent.ent_begin }}
3. 配置集群 License。

  !!! enterpriseonly

      - 此步骤只适用于创建企业版 NebulaGraph集群。
  
      - 创建社区版 NebulaGraph 集群时，忽略此步骤。

  ```bash
  kubectl create secret generic nebula-license --from-file=nebula.license
  ```

  查看集群 License 信息：

  ```bash
  kubectl get secrets nebula-license -o yaml
  ```
  {{ ent.ent_end }}

4. 查看 NebulaGraph 集群状态。
   
  ```bash
  kubectl get nebulaclusters.apps.nebula-graph.io nebula
  ```

  返回：

  ```bash
  NAME     GRAPHD-DESIRED   GRAPHD-READY   METAD-DESIRED   METAD-READY   STORAGED-DESIRED   STORAGED-READY   AGE
  nebula   1                1              1               1             3                  3                86s
  ```

## 扩缩容集群

- 不支持扩缩容社区版的 NebulaGraph 集群。

{{ ent.ent_begin }}

- 仅支持通过 v1.1.0 及以上版本的 NebulaGraph Operator 扩缩容企业版的 NebulaGraph 集群。
  
用户可以通过编辑`apps_v1alpha1_nebulacluster.yaml`文件中的`replicas`的值进行 NebulaGraph 集群的扩缩容。

### 扩容集群

本文举例扩容 NebulaGraph 集群中 Storage 服务至 5 个。步骤如下：

1. 将`apps_v1alpha1_nebulacluster.yaml`文件中`storaged.replicas`的参数值从`3`改为`5`。

  ```yaml
    storaged:
      resources:
        requests:
          cpu: "500m"
          memory: "500Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      replicas: 5
      image: vesoft/nebula-storaged
      version: {{nebula.tag}}
      dataVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
      logVolumeClaim:
        resources:
          requests:
            storage: 2Gi
        storageClassName: gp2
    reference:
      name: statefulsets.apps
      version: v1
    schedulerName: default-scheduler
  ```

2. 执行以下命令使上述更新同步至 NebulaGraph 集群 CR 中。

  ```bash
  kubectl apply -f apps_v1alpha1_nebulacluster.yaml
  ```
  
3. 查看 Storage 服务的副本数。

  ```bash
  kubectl  get pods -l app.kubernetes.io/cluster=nebula
  ```
  返回：

  ```bash
  NAME                READY   STATUS    RESTARTS   AGE
  nebula-graphd-0     1/1     Running   0          2m
  nebula-metad-0      1/1     Running   0          2m
  nebula-storaged-0   1/1     Running   0          2m
  nebula-storaged-1   1/1     Running   0          2m
  nebula-storaged-2   1/1     Running   0          2m
  nebula-storaged-3   1/1     Running   0          5m
  nebula-storaged-4   1/1     Running   0          5m
  ```
  由上可看出 Storage 服务的副本数被扩容至 5 个。

### 缩容集群

缩容集群的原理和扩容一样，用户只需将`apps_v1alpha1_nebulacluster.yaml`文件中的`replicas`的值缩小。具体操作，请参考上文的**扩容集群**部分。

!!! caution

    目前仅支持对 NebulaGraph 集群中的 Graph 服务和 Storage 服务进行扩缩容，不支持扩缩容 Meta 服务。

{{ ent.ent_end }}

## 删除集群

使用 Kubectl 删除 NebulaGraph 集群的命令如下：

```bash
kubectl delete -f apps_v1alpha1_nebulacluster.yaml
```

## 后续操作

[连接 NebulaGraph 数据库](../4.connect-to-nebula-graph-service.md)
